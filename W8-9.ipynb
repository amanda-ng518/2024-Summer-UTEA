{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8bcbce9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load package\n",
    "import numpy as np; from scipy import stats; import matplotlib.pyplot as plt; import pymc as pm;import arviz as az; \n",
    "import math; import pandas as pd\n",
    "from scipy.optimize import minimize\n",
    "from scipy.optimize import root\n",
    "from scipy import special\n",
    "import pytensor.tensor as pt\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from scipy.optimize import fsolve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a5d59e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "143e2ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_Corr_identity(p):\n",
    "\n",
    "    Sigma = np.zeros((p-1, p-1))\n",
    "    np.fill_diagonal(Sigma, 1)\n",
    "    return Sigma\n",
    "\n",
    "def generate_data(n, p, sigma_sqr, beta, nu, corr):\n",
    "\n",
    "    beta = beta.reshape((p, 1))\n",
    "    x_i = np.random.normal(0, 1, (n, p - 1))\n",
    "    x_i_correlated = x_i @ corr\n",
    "    ones = np.ones((n, 1))\n",
    "    x_i_full =  np.concatenate((ones, x_i_correlated), axis=1)\n",
    "    XB = x_i_full @ beta\n",
    "    E = stats.t.rvs(df = nu, loc=0, scale= np.sqrt(sigma_sqr), size=(n, 1))\n",
    "    Y = XB + E\n",
    "    return Y, x_i_full,x_i\n",
    "\n",
    "def calculate_y_axix(nu_origin, nu_est):\n",
    "    n = len(nu_est)\n",
    "    if n == 0:\n",
    "        return -1\n",
    "    else:\n",
    "        nu_origin_vec = nu_origin * np.ones((n, 1))\n",
    "        mse = np.sum((nu_est - nu_origin_vec)**2) / n\n",
    "        result = np.sqrt(mse)/nu_origin\n",
    "        return result\n",
    "    \n",
    "# full likelihood\n",
    "def negative_log_likelihood(params):\n",
    "    betas, sigma, nu = params[:-2], params[-2], params[-1]\n",
    "    p = X.shape[1]\n",
    "    B = np.reshape(betas, (p, 1))\n",
    "    XB = X @ B\n",
    "    n = X.shape[0]\n",
    "    XB = XB.reshape((n, 1))\n",
    "    \n",
    "    # loglikelihood\n",
    "    equation = n * np.log(special.gamma((nu + 1)/2)) + n* nu *0.5 * np.log(nu) - n * np.log(special.gamma(nu/2)) - 0.5*n*np.log(np.pi) - n * np.log(sigma) - 0.5 *(nu + 1)*np.sum(np.log(nu + ((y - XB)/sigma)**2))\n",
    "    return -equation\n",
    "\n",
    "def optimizer_all_three_params_normal(eqt, method_name):\n",
    "    p = X.shape[1]\n",
    "    bounds = [(None, None)] * p +[(0, np.inf)]*2\n",
    "    initial_guess = np.random.normal(0, 1, size = (p+2))\n",
    "    initial_guess[p] = np.abs(initial_guess[p])\n",
    "    result = minimize(eqt, initial_guess, method=method_name,bounds= bounds, options={'maxiter': 1000})\n",
    "    return result\n",
    "\n",
    "# Jeffrey's prior\n",
    "def logJeff(x):\n",
    "    return np.log((x/(x+3))**(1/2)*(special.polygamma(1,x/2) - special.polygamma(1, (x+1)/2) - 2*(x+3)/(x*(x+1)**2))**(1/2))\n",
    "\n",
    "# full joint\n",
    "def negative_joint(params):\n",
    "    nu = params[-1]\n",
    "    return -logJeff(nu) + negative_log_likelihood(params)\n",
    "\n",
    "def initial_guess_from_lin_reg(x_without_1, y,nu_origin):\n",
    "    initial_guess = []\n",
    "    \n",
    "    model = LinearRegression().fit(x_without_1, y)\n",
    "    # intercept \n",
    "    initial_guess.append(float(model.intercept_))\n",
    "    # coeff\n",
    "    for coeff in model.coef_[0]:\n",
    "        initial_guess.append(coeff)\n",
    "    # sigma_sq    \n",
    "    y_pred = model.predict(x_without_1)\n",
    "    residual_sq = (((y - y_pred)**2).sum())/(n-2)\n",
    "    initial_guess.append(residual_sq)\n",
    "    \n",
    "    # nu\n",
    "    initial_guess.append(nu_origin) # use true nu for initial guess\n",
    "    return initial_guess\n",
    "\n",
    "def optimizer_all_three_params_least_sq(eqt, initial_guess,method_name):\n",
    "    p = X.shape[1]\n",
    "    bounds = [(None, None)] * p +[(0, np.inf)]*2\n",
    "    result = minimize(eqt, initial_guess, method= method_name,bounds = bounds, options={'maxiter':1000})\n",
    "    return result\n",
    "\n",
    "def fix_x_generate_data(n, p, sigma_sqr, beta, nu, corr,X):\n",
    "\n",
    "    beta = beta.reshape((p, 1))\n",
    "    XB = X @ beta\n",
    "    E = stats.t.rvs(df = nu, loc=0, scale= np.sqrt(sigma_sqr), size=(n, 1))\n",
    "    Y = XB + E\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b459fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_func_nu(nu):\n",
    "    betas, sigma= initial_guess[:-2], initial_guess[-2]\n",
    "    p = X.shape[1]\n",
    "    B = np.reshape(betas, (p, 1))\n",
    "    XB = X @ B\n",
    "    n = X.shape[0]\n",
    "    XB = XB.reshape((n, 1))\n",
    "    Z = XB/sigma\n",
    "    return n/2*(special.gamma((nu + 1)/2) - special.gamma(nu/2)) + 1/2 * np.sum(-np.log((nu + (Z**2))/nu) + ((Z**2) - 1)/(nu + (Z**2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b895d21b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_func_all_param(param):\n",
    "    betas, sigma, nu = param[:-2],param[-2],param[-1]\n",
    "    \n",
    "    p = X.shape[1]\n",
    "    B = np.reshape(betas, (p, 1))\n",
    "    XB = X @ B\n",
    "    n = X.shape[0]\n",
    "    XB = XB.reshape((n, 1))\n",
    "    Z = XB/sigma\n",
    "    \n",
    "    beta0_eqt = (nu+1)/2 *np.sum(1/(nu + (Z**2)) * 2*Z* X[:,0])\n",
    "    beta1_eqt = (nu+1)/2 *np.sum(1/(nu + (Z**2)) * 2*Z* X[:,1])\n",
    "    beta2_eqt = (nu+1)/2 *np.sum(1/(nu + (Z**2)) * 2*Z* X[:,2])\n",
    "    beta3_eqt = (nu+1)/2 *np.sum(1/(nu + (Z**2)) * 2*Z* X[:,3])\n",
    "    beta4_eqt = (nu+1)/2 *np.sum(1/(nu + (Z**2)) * 2*Z* X[:,4])\n",
    "    \n",
    "    sigma_eqt = -n/2 + (nu+1)/2 * np.sum(1/(nu + (Z**2)) *2*Z * (y-XB)/(sigma**2))\n",
    "    nu_eqt = n/2*(special.gamma((nu + 1)/2) - special.gamma(nu/2)) + 1/2 * np.sum(-np.log((nu + (Z**2))/nu) + ((Z**2) - 1)/(nu + (Z**2)))\n",
    "    return [beta0_eqt,beta1_eqt,beta2_eqt,beta3_eqt,beta4_eqt,sigma_eqt,nu_eqt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1f9a5e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix number of observations\n",
    "n = 50\n",
    "method_list = ['Nelder-Mead', 'L-BFGS-B']#,'BFGS']\n",
    "\n",
    "p = 5\n",
    "beta = np.array([2, 1, 0.3, 0.9, 1])\n",
    "sigma_sqr = 1.5\n",
    "corr = generate_Corr_identity(p)\n",
    "nu_origin = 20\n",
    "\n",
    "x_without_1 = np.random.normal(0, 1, (n, p - 1))\n",
    "x_i_correlated = x_without_1 @ corr\n",
    "ones = np.ones((n, 1))\n",
    "X =  np.concatenate((ones, x_i_correlated), axis=1)\n",
    "\n",
    "estimates_dict = {}\n",
    "\n",
    "for method_name in method_list:\n",
    "    estimates_dict[method_name] = {}\n",
    "    estimates_dict[method_name]['normal_mle_profile'] = []\n",
    "    estimates_dict[method_name]['normal_map_profile'] = []\n",
    "    estimates_dict[method_name]['lse_mle_profile'] = []\n",
    "    estimates_dict[method_name]['lse_map_profile'] = []\n",
    "    estimates_dict[method_name]['root_mle_profile'] = []\n",
    "    estimates_dict[method_name]['all_param_root_mle_profile'] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9de1e9fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(50): # number of simulations \n",
    "    y = fix_x_generate_data(n, p, sigma_sqr, beta, nu_origin, corr, X) ## Generate data   \n",
    "    \n",
    "    for method_name in method_list:\n",
    "        \n",
    "        # Normal(0,1) as initial guess\n",
    "        profile_normal_MLE_result = optimizer_all_three_params_normal(negative_log_likelihood,method_name)\n",
    "        if profile_normal_MLE_result.success == True:\n",
    "            estimates_dict[method_name]['normal_mle_profile'].append(profile_normal_MLE_result.x[-1])\n",
    "        \n",
    "        profile_normal_joint_result = optimizer_all_three_params_normal(negative_joint,method_name)\n",
    "        if profile_normal_joint_result.success == True:\n",
    "            estimates_dict[method_name]['normal_map_profile'].append(profile_normal_joint_result.x[-1])\n",
    "        \n",
    "        # Least square estimates as initial guess\n",
    "        initial_guess = initial_guess_from_lin_reg(x_without_1, y,nu_origin)\n",
    "        profile_lse_MLE_result = optimizer_all_three_params_least_sq(negative_log_likelihood, initial_guess, method_name)\n",
    "        if profile_lse_MLE_result.success == True:\n",
    "            estimates_dict[method_name]['lse_mle_profile'].append(profile_lse_MLE_result.x[-1])\n",
    "        \n",
    "        profile_lse_joint_result = optimizer_all_three_params_least_sq(negative_joint, initial_guess, method_name)\n",
    "        if profile_lse_joint_result.success == True:\n",
    "            estimates_dict[method_name]['lse_map_profile'].append(profile_lse_joint_result.x[-1])\n",
    "        \n",
    "        # Root solver\n",
    "        profile_root_MLE_result = root(score_func_nu, x0 = initial_guess[-1], method = 'broyden1')\n",
    "        if profile_root_MLE_result.success == True:\n",
    "            estimates_dict[method_name]['root_mle_profile'].append(profile_root_MLE_result.x)\n",
    "            \n",
    "        profile_all_param_root_MLE_result = fsolve(score_func_all_param, x0 = initial_guess)\n",
    "        estimates_dict[method_name]['all_param_root_mle_profile'].append(profile_all_param_root_MLE_result[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "46e443d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_dict = {}\n",
    "mse_list = ['normal_mle_profile','normal_map_profile','lse_mle_profile','lse_map_profile','root_mle_profile','all_param_root_mle_profile']\n",
    "\n",
    "for method_name in method_list:\n",
    "    final_dict[method_name] = {}\n",
    "    final_dict[method_name][\"MSE\"] = []\n",
    "    final_dict[method_name][\"Number of convergence\"] = []\n",
    "    \n",
    "    for mse in mse_list:\n",
    "        final_dict[method_name][\"MSE\"].append(calculate_y_axix(nu_origin, estimates_dict[method_name][mse]))\n",
    "        final_dict[method_name][\"Number of convergence\"].append(len(estimates_dict[method_name][mse]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a6e2b8ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Nelder-Mead': {'MSE': [4.774334944338542,\n",
       "   4.761360982988759,\n",
       "   11.9301380394459,\n",
       "   5.677647423419091,\n",
       "   5.670330294619745,\n",
       "   5.354400415899868],\n",
       "  'Number of convergence': [25, 26, 28, 50, 50, 50]},\n",
       " 'L-BFGS-B': {'MSE': [9.741406354072101,\n",
       "   1.8217946529132596,\n",
       "   33.961582052133444,\n",
       "   3.9255166681251885,\n",
       "   5.670330294619745,\n",
       "   5.354400415899868],\n",
       "  'Number of convergence': [7, 6, 47, 26, 50, 50]}}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5962bdcf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
